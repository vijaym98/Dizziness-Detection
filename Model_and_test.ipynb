{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_and_test.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMyvL0ZOXUyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0zQ0GITXndF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/dataset1.csv')\n",
        "df = df.drop([\"Unnamed: 0\"], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNgbd8mPZMfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.drop([\"y\"], axis = 1)\n",
        "y = df[\"y\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5jWEOAecS5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(y)):\n",
        "  if y[i] == 10: y[i]=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV8yE3LWcwM8",
        "colab_type": "code",
        "outputId": "f34f774d-ed32-417c-9202-6072ce073427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "y"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       0.0\n",
              "1       0.0\n",
              "2       0.0\n",
              "3       0.0\n",
              "4       0.0\n",
              "       ... \n",
              "5711    1.0\n",
              "5712    1.0\n",
              "5713    1.0\n",
              "5714    1.0\n",
              "5715    1.0\n",
              "Name: y, Length: 5716, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3o-DUOyc0cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_a, y_train, y_a = train_test_split(X,y,test_size = 2/12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XghemU7WxcJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_b,X_test,y_b,y_test = train_test_split(X_a,y_a,test_size = 1/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcXtnVwIayhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_shaped = np.expand_dims(X_train, axis=2)\n",
        "X_test_shaped = np.expand_dims(X_test, axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wCNpxSn4G9O",
        "colab_type": "text"
      },
      "source": [
        "# CNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmstcRI_ZkiU",
        "colab_type": "code",
        "outputId": "6e29fdc0-3c82-4a7c-f15a-74300ff727d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcAzHYRibIX8",
        "colab_type": "code",
        "outputId": "2bdd1e8f-876e-497e-ac8c-58bbdeec009a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4763, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agxywlDhZl6w",
        "colab_type": "code",
        "outputId": "402f35db-b798-45e8-d17c-41f3133fec0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(64, kernel_size = 3, activation = 'relu', input_shape = (4,1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation = 'relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1,activation = 'sigmoid'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StusOevBaoJi",
        "colab_type": "code",
        "outputId": "de33f03e-04e6-43b1-8157-c254fc870977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "optimizer = Adam(lr=0.00001)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N_1vx3HasvJ",
        "colab_type": "code",
        "outputId": "24998702-1aaa-4b38-c0fb-dc8b347bbe5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train_shaped, y_train, validation_data = (X_test_shaped,y_test), epochs = 100)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 4763 samples, validate on 477 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "4763/4763 [==============================] - 1s 125us/step - loss: 0.9472 - acc: 0.5062 - val_loss: 0.9014 - val_acc: 0.4906\n",
            "Epoch 2/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.9222 - acc: 0.5062 - val_loss: 0.8747 - val_acc: 0.4969\n",
            "Epoch 3/100\n",
            "4763/4763 [==============================] - 0s 53us/step - loss: 0.9096 - acc: 0.5112 - val_loss: 0.8503 - val_acc: 0.5052\n",
            "Epoch 4/100\n",
            "4763/4763 [==============================] - 0s 53us/step - loss: 0.8879 - acc: 0.5159 - val_loss: 0.8289 - val_acc: 0.5241\n",
            "Epoch 5/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.8620 - acc: 0.5339 - val_loss: 0.8099 - val_acc: 0.5535\n",
            "Epoch 6/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.8540 - acc: 0.5289 - val_loss: 0.7888 - val_acc: 0.5597\n",
            "Epoch 7/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.8055 - acc: 0.5413 - val_loss: 0.7701 - val_acc: 0.5639\n",
            "Epoch 8/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.8007 - acc: 0.5379 - val_loss: 0.7514 - val_acc: 0.5702\n",
            "Epoch 9/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.7668 - acc: 0.5543 - val_loss: 0.7365 - val_acc: 0.5891\n",
            "Epoch 10/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.7642 - acc: 0.5488 - val_loss: 0.7230 - val_acc: 0.6164\n",
            "Epoch 11/100\n",
            "4763/4763 [==============================] - 0s 52us/step - loss: 0.7500 - acc: 0.5696 - val_loss: 0.7109 - val_acc: 0.6247\n",
            "Epoch 12/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.7321 - acc: 0.5646 - val_loss: 0.6990 - val_acc: 0.6268\n",
            "Epoch 13/100\n",
            "4763/4763 [==============================] - 0s 52us/step - loss: 0.7187 - acc: 0.5748 - val_loss: 0.6897 - val_acc: 0.6289\n",
            "Epoch 14/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.7080 - acc: 0.5816 - val_loss: 0.6816 - val_acc: 0.6247\n",
            "Epoch 15/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6939 - acc: 0.5860 - val_loss: 0.6753 - val_acc: 0.6289\n",
            "Epoch 16/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6829 - acc: 0.5933 - val_loss: 0.6689 - val_acc: 0.6310\n",
            "Epoch 17/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6790 - acc: 0.6009 - val_loss: 0.6645 - val_acc: 0.6247\n",
            "Epoch 18/100\n",
            "4763/4763 [==============================] - 0s 52us/step - loss: 0.6666 - acc: 0.6024 - val_loss: 0.6601 - val_acc: 0.6289\n",
            "Epoch 19/100\n",
            "4763/4763 [==============================] - 0s 55us/step - loss: 0.6635 - acc: 0.5982 - val_loss: 0.6576 - val_acc: 0.6310\n",
            "Epoch 20/100\n",
            "4763/4763 [==============================] - 0s 52us/step - loss: 0.6600 - acc: 0.6063 - val_loss: 0.6545 - val_acc: 0.6268\n",
            "Epoch 21/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6513 - acc: 0.6139 - val_loss: 0.6533 - val_acc: 0.6310\n",
            "Epoch 22/100\n",
            "4763/4763 [==============================] - 0s 54us/step - loss: 0.6434 - acc: 0.6196 - val_loss: 0.6514 - val_acc: 0.6310\n",
            "Epoch 23/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6469 - acc: 0.6126 - val_loss: 0.6494 - val_acc: 0.6331\n",
            "Epoch 24/100\n",
            "4763/4763 [==============================] - 0s 52us/step - loss: 0.6465 - acc: 0.6244 - val_loss: 0.6490 - val_acc: 0.6331\n",
            "Epoch 25/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6508 - acc: 0.6097 - val_loss: 0.6477 - val_acc: 0.6331\n",
            "Epoch 26/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6421 - acc: 0.6210 - val_loss: 0.6449 - val_acc: 0.6415\n",
            "Epoch 27/100\n",
            "4763/4763 [==============================] - 0s 53us/step - loss: 0.6413 - acc: 0.6194 - val_loss: 0.6432 - val_acc: 0.6310\n",
            "Epoch 28/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6399 - acc: 0.6206 - val_loss: 0.6418 - val_acc: 0.6352\n",
            "Epoch 29/100\n",
            "4763/4763 [==============================] - 0s 52us/step - loss: 0.6372 - acc: 0.6254 - val_loss: 0.6408 - val_acc: 0.6373\n",
            "Epoch 30/100\n",
            "4763/4763 [==============================] - 0s 59us/step - loss: 0.6390 - acc: 0.6244 - val_loss: 0.6398 - val_acc: 0.6331\n",
            "Epoch 31/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6367 - acc: 0.6236 - val_loss: 0.6390 - val_acc: 0.6331\n",
            "Epoch 32/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.6408 - acc: 0.6133 - val_loss: 0.6378 - val_acc: 0.6373\n",
            "Epoch 33/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6325 - acc: 0.6267 - val_loss: 0.6376 - val_acc: 0.6352\n",
            "Epoch 34/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6362 - acc: 0.6309 - val_loss: 0.6363 - val_acc: 0.6352\n",
            "Epoch 35/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6331 - acc: 0.6196 - val_loss: 0.6349 - val_acc: 0.6352\n",
            "Epoch 36/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6253 - acc: 0.6399 - val_loss: 0.6353 - val_acc: 0.6373\n",
            "Epoch 37/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.6349 - acc: 0.6328 - val_loss: 0.6347 - val_acc: 0.6373\n",
            "Epoch 38/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6289 - acc: 0.6334 - val_loss: 0.6336 - val_acc: 0.6373\n",
            "Epoch 39/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6300 - acc: 0.6357 - val_loss: 0.6327 - val_acc: 0.6331\n",
            "Epoch 40/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.6260 - acc: 0.6376 - val_loss: 0.6320 - val_acc: 0.6331\n",
            "Epoch 41/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6283 - acc: 0.6366 - val_loss: 0.6318 - val_acc: 0.6331\n",
            "Epoch 42/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6272 - acc: 0.6296 - val_loss: 0.6301 - val_acc: 0.6331\n",
            "Epoch 43/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6269 - acc: 0.6305 - val_loss: 0.6297 - val_acc: 0.6373\n",
            "Epoch 44/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6207 - acc: 0.6456 - val_loss: 0.6290 - val_acc: 0.6436\n",
            "Epoch 45/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6218 - acc: 0.6378 - val_loss: 0.6281 - val_acc: 0.6373\n",
            "Epoch 46/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6216 - acc: 0.6425 - val_loss: 0.6275 - val_acc: 0.6352\n",
            "Epoch 47/100\n",
            "4763/4763 [==============================] - 0s 52us/step - loss: 0.6279 - acc: 0.6376 - val_loss: 0.6270 - val_acc: 0.6352\n",
            "Epoch 48/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6193 - acc: 0.6494 - val_loss: 0.6264 - val_acc: 0.6373\n",
            "Epoch 49/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6163 - acc: 0.6513 - val_loss: 0.6257 - val_acc: 0.6394\n",
            "Epoch 50/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.6231 - acc: 0.6473 - val_loss: 0.6254 - val_acc: 0.6373\n",
            "Epoch 51/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.6175 - acc: 0.6590 - val_loss: 0.6244 - val_acc: 0.6352\n",
            "Epoch 52/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6179 - acc: 0.6584 - val_loss: 0.6238 - val_acc: 0.6394\n",
            "Epoch 53/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6219 - acc: 0.6492 - val_loss: 0.6238 - val_acc: 0.6436\n",
            "Epoch 54/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6154 - acc: 0.6620 - val_loss: 0.6234 - val_acc: 0.6394\n",
            "Epoch 55/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6232 - acc: 0.6513 - val_loss: 0.6232 - val_acc: 0.6415\n",
            "Epoch 56/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6141 - acc: 0.6580 - val_loss: 0.6228 - val_acc: 0.6373\n",
            "Epoch 57/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6178 - acc: 0.6588 - val_loss: 0.6220 - val_acc: 0.6394\n",
            "Epoch 58/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.6158 - acc: 0.6601 - val_loss: 0.6211 - val_acc: 0.6394\n",
            "Epoch 59/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6137 - acc: 0.6643 - val_loss: 0.6209 - val_acc: 0.6373\n",
            "Epoch 60/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6155 - acc: 0.6574 - val_loss: 0.6209 - val_acc: 0.6394\n",
            "Epoch 61/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.6122 - acc: 0.6620 - val_loss: 0.6199 - val_acc: 0.6415\n",
            "Epoch 62/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6108 - acc: 0.6609 - val_loss: 0.6191 - val_acc: 0.6415\n",
            "Epoch 63/100\n",
            "4763/4763 [==============================] - 0s 54us/step - loss: 0.6093 - acc: 0.6666 - val_loss: 0.6188 - val_acc: 0.6373\n",
            "Epoch 64/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.6126 - acc: 0.6622 - val_loss: 0.6177 - val_acc: 0.6457\n",
            "Epoch 65/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6142 - acc: 0.6571 - val_loss: 0.6178 - val_acc: 0.6457\n",
            "Epoch 66/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6127 - acc: 0.6649 - val_loss: 0.6175 - val_acc: 0.6457\n",
            "Epoch 67/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.6076 - acc: 0.6618 - val_loss: 0.6166 - val_acc: 0.6478\n",
            "Epoch 68/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.6096 - acc: 0.6655 - val_loss: 0.6157 - val_acc: 0.6520\n",
            "Epoch 69/100\n",
            "4763/4763 [==============================] - 0s 52us/step - loss: 0.6043 - acc: 0.6714 - val_loss: 0.6154 - val_acc: 0.6499\n",
            "Epoch 70/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6061 - acc: 0.6695 - val_loss: 0.6156 - val_acc: 0.6457\n",
            "Epoch 71/100\n",
            "4763/4763 [==============================] - 0s 54us/step - loss: 0.6060 - acc: 0.6647 - val_loss: 0.6151 - val_acc: 0.6499\n",
            "Epoch 72/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.6070 - acc: 0.6674 - val_loss: 0.6146 - val_acc: 0.6541\n",
            "Epoch 73/100\n",
            "4763/4763 [==============================] - 0s 53us/step - loss: 0.6091 - acc: 0.6691 - val_loss: 0.6140 - val_acc: 0.6562\n",
            "Epoch 74/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6024 - acc: 0.6771 - val_loss: 0.6134 - val_acc: 0.6583\n",
            "Epoch 75/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.6069 - acc: 0.6704 - val_loss: 0.6133 - val_acc: 0.6604\n",
            "Epoch 76/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6033 - acc: 0.6807 - val_loss: 0.6129 - val_acc: 0.6583\n",
            "Epoch 77/100\n",
            "4763/4763 [==============================] - 0s 49us/step - loss: 0.6032 - acc: 0.6769 - val_loss: 0.6121 - val_acc: 0.6604\n",
            "Epoch 78/100\n",
            "4763/4763 [==============================] - 0s 56us/step - loss: 0.6046 - acc: 0.6727 - val_loss: 0.6118 - val_acc: 0.6583\n",
            "Epoch 79/100\n",
            "4763/4763 [==============================] - 0s 59us/step - loss: 0.6014 - acc: 0.6805 - val_loss: 0.6111 - val_acc: 0.6604\n",
            "Epoch 80/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6088 - acc: 0.6637 - val_loss: 0.6106 - val_acc: 0.6646\n",
            "Epoch 81/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6026 - acc: 0.6689 - val_loss: 0.6105 - val_acc: 0.6646\n",
            "Epoch 82/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6026 - acc: 0.6769 - val_loss: 0.6104 - val_acc: 0.6646\n",
            "Epoch 83/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.6021 - acc: 0.6710 - val_loss: 0.6100 - val_acc: 0.6667\n",
            "Epoch 84/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.5980 - acc: 0.6828 - val_loss: 0.6092 - val_acc: 0.6688\n",
            "Epoch 85/100\n",
            "4763/4763 [==============================] - 0s 52us/step - loss: 0.6017 - acc: 0.6737 - val_loss: 0.6088 - val_acc: 0.6688\n",
            "Epoch 86/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6024 - acc: 0.6773 - val_loss: 0.6083 - val_acc: 0.6709\n",
            "Epoch 87/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.5982 - acc: 0.6763 - val_loss: 0.6076 - val_acc: 0.6730\n",
            "Epoch 88/100\n",
            "4763/4763 [==============================] - 0s 52us/step - loss: 0.6011 - acc: 0.6763 - val_loss: 0.6077 - val_acc: 0.6751\n",
            "Epoch 89/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.5992 - acc: 0.6733 - val_loss: 0.6073 - val_acc: 0.6751\n",
            "Epoch 90/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.5956 - acc: 0.6805 - val_loss: 0.6066 - val_acc: 0.6730\n",
            "Epoch 91/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.6014 - acc: 0.6710 - val_loss: 0.6061 - val_acc: 0.6771\n",
            "Epoch 92/100\n",
            "4763/4763 [==============================] - 0s 52us/step - loss: 0.6010 - acc: 0.6729 - val_loss: 0.6058 - val_acc: 0.6813\n",
            "Epoch 93/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.5977 - acc: 0.6830 - val_loss: 0.6053 - val_acc: 0.6813\n",
            "Epoch 94/100\n",
            "4763/4763 [==============================] - 0s 48us/step - loss: 0.5930 - acc: 0.6872 - val_loss: 0.6051 - val_acc: 0.6792\n",
            "Epoch 95/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.5954 - acc: 0.6828 - val_loss: 0.6048 - val_acc: 0.6771\n",
            "Epoch 96/100\n",
            "4763/4763 [==============================] - 0s 50us/step - loss: 0.5964 - acc: 0.6834 - val_loss: 0.6044 - val_acc: 0.6771\n",
            "Epoch 97/100\n",
            "4763/4763 [==============================] - 0s 48us/step - loss: 0.5918 - acc: 0.6878 - val_loss: 0.6045 - val_acc: 0.6792\n",
            "Epoch 98/100\n",
            "4763/4763 [==============================] - 0s 51us/step - loss: 0.5985 - acc: 0.6775 - val_loss: 0.6041 - val_acc: 0.6813\n",
            "Epoch 99/100\n",
            "4763/4763 [==============================] - 0s 53us/step - loss: 0.5958 - acc: 0.6832 - val_loss: 0.6037 - val_acc: 0.6813\n",
            "Epoch 100/100\n",
            "4763/4763 [==============================] - 0s 52us/step - loss: 0.5912 - acc: 0.6859 - val_loss: 0.6034 - val_acc: 0.6834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Yvisgs6dXst",
        "colab_type": "code",
        "outputId": "09966017-6d9a-412a-d747-6cc8882c7be9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 2, 64)             256       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 4,929\n",
            "Trainable params: 4,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW5XV2mCdj2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_b_shaped = np.expand_dims(X_b, axis=2)\n",
        "count=0\n",
        "test_pred = model.predict_classes(X_b_shaped)\n",
        "k1 = np.array(test_pred[:,0])\n",
        "k2 = np.array(y_b)\n",
        "for i in range(len(y_b)):\n",
        "  if k1[i] == k2[i]:\n",
        "    count = count+1\n",
        "accuracy = (count*100)/len(y_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0csklytfHvZ",
        "colab_type": "code",
        "outputId": "1e375e39-082e-4a86-cd72-37c155845258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69.74789915966386"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgABEtsU0uGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PQUGJ0omWd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6JZCHx4mxHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi2-05mNo7hR",
        "colab_type": "code",
        "outputId": "fe78b92a-daeb-459b-e7ac-371cc6438808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "acc = accuracy_score(k2, k1)\n",
        "f1 = f1_score(k2, test_pred[:,0])\n",
        "y_score = model.predict_proba(X_b_shaped)\n",
        "roc = roc_auc_score(k2, y_score)\n",
        "print('The accuracy of this model is ' + str(acc))\n",
        "print('The f1 score of this model is ' + str(f1))\n",
        "print('The area under curve of this model is ' + str(roc))\n",
        "print('The confusion matrix is as follows:')\n",
        "print(confusion_matrix(y_b, test_pred[:,0]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of this model is 0.6974789915966386\n",
            "The f1 score of this model is 0.6785714285714285\n",
            "The area under curve of this model is 0.7801885460066379\n",
            "The confusion matrix is as follows:\n",
            "[[180  58]\n",
            " [ 86 152]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYlVfkgoqItD",
        "colab_type": "code",
        "outputId": "997bd6d7-e1ec-453d-8cb0-9146f4149dbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "fpr, tpr,x = roc_curve( k2, y_score)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.title('ROC Curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'True Positive Rate')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T6QWIg74RwY",
        "colab_type": "text"
      },
      "source": [
        "# Testing on New files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCQlypEUDitX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TJStSOTDdf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial import distance\n",
        "from mlxtend.image import extract_face_landmarks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2L1fBaYDXHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def images(x):\n",
        "    vc.set(cv2.CAP_PROP_POS_MSEC, x)\n",
        "    result,image = vc.read()\n",
        "    return result, image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IQbusxCC8Cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/Untitled folder/Video/'\n",
        "filename = 'test5.mp4'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_BvncH-DEEr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "f4871ea6-52e8-4618-c6da-a9b5257a0ff8"
      },
      "source": [
        "lm =[]\n",
        "frames_max = 300 #number of frames needed per video\n",
        "vc = cv2.VideoCapture(path + filename)\n",
        "tim = 0\n",
        "result, image  = images(tim)\n",
        "count = 0\n",
        "while result and count < frames_max: \n",
        "    landmarks = extract_face_landmarks(image)\n",
        "    if sum(sum(landmarks)) != 0:\n",
        "        lm.append(landmarks)\n",
        "        tim +=600 \n",
        "        result, image = images(tim)\n",
        "        count += 1\n",
        "        if count%10 == 0: print('Extracted face landmarks from '+str(count)+' frames')\n",
        "    else:  \n",
        "        tim += 600\n",
        "        result, image = images(tim)\n",
        "        print('Face not detected in this frame, till now extracted from '+str(count)+' frames')\n",
        "lm = np.array(lm)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracted face landmarks from 10 frames\n",
            "Extracted face landmarks from 20 frames\n",
            "Extracted face landmarks from 30 frames\n",
            "Extracted face landmarks from 40 frames\n",
            "Extracted face landmarks from 50 frames\n",
            "Extracted face landmarks from 60 frames\n",
            "Extracted face landmarks from 70 frames\n",
            "Extracted face landmarks from 80 frames\n",
            "Extracted face landmarks from 90 frames\n",
            "Extracted face landmarks from 100 frames\n",
            "Extracted face landmarks from 110 frames\n",
            "Extracted face landmarks from 120 frames\n",
            "Extracted face landmarks from 130 frames\n",
            "Extracted face landmarks from 140 frames\n",
            "Extracted face landmarks from 150 frames\n",
            "Extracted face landmarks from 160 frames\n",
            "Extracted face landmarks from 170 frames\n",
            "Extracted face landmarks from 180 frames\n",
            "Extracted face landmarks from 190 frames\n",
            "Extracted face landmarks from 200 frames\n",
            "Extracted face landmarks from 210 frames\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovH38VkL2tZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def EAR(arr):\n",
        "\tx1,x2,x3 = distance.euclidean(arr[1], arr[5]),distance.euclidean(arr[2], arr[4]),distance.euclidean(arr[0], arr[3])\n",
        "\tfrac = (x1+ x2) / (2.0 * x3)\n",
        "\treturn frac\n",
        "\n",
        "def MAR(arr):\n",
        "    x1,x2 = distance.euclidean(arr[14], arr[18]), distance.euclidean(arr[12], arr[16])\n",
        "    frac = x1/x2\n",
        "    return frac\n",
        "\n",
        "def PUC(arr):\n",
        "    r = distance.euclidean(arr[1], arr[4])/2.0\n",
        "    area = math.pi*(r**2)\n",
        "    peri = 0\n",
        "    for i in range(0,5):\n",
        "      peri = peri + distance.euclidean(arr[i], arr[i+1])\n",
        "    peri = peri + distance.euclidean(arr[5], arr[0])\n",
        "    return 4 * math.pi * area /(peri**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtE4Dv3vDRw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = []\n",
        "for l in lm:\n",
        "  mainMarks = l[36:68]\n",
        "  x1, x2, x3 = EAR(mainMarks), MAR(mainMarks), PUC(mainMarks)\n",
        "  features.append([x1, x2, x3, x2/x1])\n",
        "features = np.array(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhFwITqPDyC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savetxt(\"test5.csv\", features, delimiter = \",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2aikOVCD6XW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2jWUjF1EmsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv('/content/test5.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQrd9Lh3Ex0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.columns = ['x1','x2','x3','x4']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnqitYkWGyGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_temp = df.iloc[0:3, :]\n",
        "df_mean = df_temp.mean()\n",
        "df_std = df_temp.std()\n",
        "df[\"x1_norm\"]=(df[\"x1\"]-df_mean[\"x1\"])/df_std[\"x1\"]\n",
        "df[\"x2_norm\"]=(df[\"x2\"]-df_mean[\"x2\"])/df_std[\"x2\"]\n",
        "df[\"x3_norm\"]=(df[\"x3\"]-df_mean[\"x3\"])/df_std[\"x3\"]\n",
        "df[\"x4_norm\"]=(df[\"x4\"]-df_mean[\"x4\"])/df_std[\"x4\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-Gh6VhBHCKm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "eeae9934-6084-47ea-ed22-1ce174a5fed9"
      },
      "source": [
        "df"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x1_norm</th>\n",
              "      <th>x2_norm</th>\n",
              "      <th>x3_norm</th>\n",
              "      <th>x4_norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.299626</td>\n",
              "      <td>0.970143</td>\n",
              "      <td>0.431116</td>\n",
              "      <td>3.237848</td>\n",
              "      <td>-0.854937</td>\n",
              "      <td>-0.966736</td>\n",
              "      <td>-1.113662</td>\n",
              "      <td>0.870440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.315353</td>\n",
              "      <td>0.977050</td>\n",
              "      <td>0.467124</td>\n",
              "      <td>3.098273</td>\n",
              "      <td>-0.244701</td>\n",
              "      <td>-0.063502</td>\n",
              "      <td>0.292600</td>\n",
              "      <td>0.221857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.985414</td>\n",
              "      <td>0.480655</td>\n",
              "      <td>2.815469</td>\n",
              "      <td>1.099638</td>\n",
              "      <td>1.030238</td>\n",
              "      <td>0.821062</td>\n",
              "      <td>-1.092297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.987916</td>\n",
              "      <td>0.507924</td>\n",
              "      <td>2.963749</td>\n",
              "      <td>0.452954</td>\n",
              "      <td>1.357455</td>\n",
              "      <td>1.886043</td>\n",
              "      <td>-0.403259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.302069</td>\n",
              "      <td>0.959827</td>\n",
              "      <td>0.437804</td>\n",
              "      <td>3.177507</td>\n",
              "      <td>-0.760132</td>\n",
              "      <td>-2.315702</td>\n",
              "      <td>-0.852469</td>\n",
              "      <td>0.590044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>0.122859</td>\n",
              "      <td>1.591693</td>\n",
              "      <td>0.236961</td>\n",
              "      <td>12.955444</td>\n",
              "      <td>-7.713667</td>\n",
              "      <td>80.310241</td>\n",
              "      <td>-8.696348</td>\n",
              "      <td>46.026725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>0.186052</td>\n",
              "      <td>1.490875</td>\n",
              "      <td>0.397950</td>\n",
              "      <td>8.013211</td>\n",
              "      <td>-5.261710</td>\n",
              "      <td>67.126726</td>\n",
              "      <td>-2.408936</td>\n",
              "      <td>23.060871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>0.217061</td>\n",
              "      <td>1.547742</td>\n",
              "      <td>0.390329</td>\n",
              "      <td>7.130455</td>\n",
              "      <td>-4.058541</td>\n",
              "      <td>74.562980</td>\n",
              "      <td>-2.706572</td>\n",
              "      <td>18.958829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>0.218750</td>\n",
              "      <td>1.413513</td>\n",
              "      <td>0.397307</td>\n",
              "      <td>6.461775</td>\n",
              "      <td>-3.992998</td>\n",
              "      <td>57.010571</td>\n",
              "      <td>-2.434059</td>\n",
              "      <td>15.851570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>0.314980</td>\n",
              "      <td>1.542633</td>\n",
              "      <td>0.408557</td>\n",
              "      <td>4.897563</td>\n",
              "      <td>-0.259183</td>\n",
              "      <td>73.894907</td>\n",
              "      <td>-1.994707</td>\n",
              "      <td>8.582898</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>122 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           x1        x2        x3  ...    x2_norm   x3_norm    x4_norm\n",
              "0    0.299626  0.970143  0.431116  ...  -0.966736 -1.113662   0.870440\n",
              "1    0.315353  0.977050  0.467124  ...  -0.063502  0.292600   0.221857\n",
              "2    0.350000  0.985414  0.480655  ...   1.030238  0.821062  -1.092297\n",
              "3    0.333333  0.987916  0.507924  ...   1.357455  1.886043  -0.403259\n",
              "4    0.302069  0.959827  0.437804  ...  -2.315702 -0.852469   0.590044\n",
              "..        ...       ...       ...  ...        ...       ...        ...\n",
              "117  0.122859  1.591693  0.236961  ...  80.310241 -8.696348  46.026725\n",
              "118  0.186052  1.490875  0.397950  ...  67.126726 -2.408936  23.060871\n",
              "119  0.217061  1.547742  0.390329  ...  74.562980 -2.706572  18.958829\n",
              "120  0.218750  1.413513  0.397307  ...  57.010571 -2.434059  15.851570\n",
              "121  0.314980  1.542633  0.408557  ...  73.894907 -1.994707   8.582898\n",
              "\n",
              "[122 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzWABLjNHFi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop([\"x1\",\"x2\",\"x3\",\"x4\"],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJXt3OTwHUyU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "200b5528-64ec-41b3-8f30-283e9aaa4b63"
      },
      "source": [
        "df"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1_norm</th>\n",
              "      <th>x2_norm</th>\n",
              "      <th>x3_norm</th>\n",
              "      <th>x4_norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.854937</td>\n",
              "      <td>-0.966736</td>\n",
              "      <td>-1.113662</td>\n",
              "      <td>0.870440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.244701</td>\n",
              "      <td>-0.063502</td>\n",
              "      <td>0.292600</td>\n",
              "      <td>0.221857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.099638</td>\n",
              "      <td>1.030238</td>\n",
              "      <td>0.821062</td>\n",
              "      <td>-1.092297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.452954</td>\n",
              "      <td>1.357455</td>\n",
              "      <td>1.886043</td>\n",
              "      <td>-0.403259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.760132</td>\n",
              "      <td>-2.315702</td>\n",
              "      <td>-0.852469</td>\n",
              "      <td>0.590044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>-7.713667</td>\n",
              "      <td>80.310241</td>\n",
              "      <td>-8.696348</td>\n",
              "      <td>46.026725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>-5.261710</td>\n",
              "      <td>67.126726</td>\n",
              "      <td>-2.408936</td>\n",
              "      <td>23.060871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>-4.058541</td>\n",
              "      <td>74.562980</td>\n",
              "      <td>-2.706572</td>\n",
              "      <td>18.958829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>-3.992998</td>\n",
              "      <td>57.010571</td>\n",
              "      <td>-2.434059</td>\n",
              "      <td>15.851570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>-0.259183</td>\n",
              "      <td>73.894907</td>\n",
              "      <td>-1.994707</td>\n",
              "      <td>8.582898</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>122 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      x1_norm    x2_norm   x3_norm    x4_norm\n",
              "0   -0.854937  -0.966736 -1.113662   0.870440\n",
              "1   -0.244701  -0.063502  0.292600   0.221857\n",
              "2    1.099638   1.030238  0.821062  -1.092297\n",
              "3    0.452954   1.357455  1.886043  -0.403259\n",
              "4   -0.760132  -2.315702 -0.852469   0.590044\n",
              "..        ...        ...       ...        ...\n",
              "117 -7.713667  80.310241 -8.696348  46.026725\n",
              "118 -5.261710  67.126726 -2.408936  23.060871\n",
              "119 -4.058541  74.562980 -2.706572  18.958829\n",
              "120 -3.992998  57.010571 -2.434059  15.851570\n",
              "121 -0.259183  73.894907 -1.994707   8.582898\n",
              "\n",
              "[122 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD15M1QbHVVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = np.expand_dims(df, axis=2)\n",
        "tp = model.predict_classes(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWCd1GdWITkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tp = np.array(tp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuMmguW1IUNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val = []\n",
        "for i in range(len(tp)): val.append(tp[i][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WqupK6X4jYt",
        "colab_type": "text"
      },
      "source": [
        "# Output graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRF-Qw-UIjzW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "4a0b00a5-cbfb-48f9-a0ad-40a767918369"
      },
      "source": [
        "x = np.arange(0,(len(val)*0.6)-1,0.6)\n",
        "plt.plot(x,val[:len(val)-1])\n",
        "plt.xlabel('time (in seconds)')\n",
        "plt.ylabel('states (state = 1 means the person feels dizzy)')\n",
        "plt.title('label vs time')"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'label vs time')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnDnHBYHjOM6c44LAcRxnzvn/hJfHl2Ygf+wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J9W2KqV4tSJ",
        "colab_type": "text"
      },
      "source": [
        "# Generating JSON file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuGH_aGKnpp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fin=[]\n",
        "for i in range(len(val)):\n",
        "  fin.append([str(i*0.6),str(val[i])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amBMM2Z0zSK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dt = {}\n",
        "dt['dizzy']=fin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nlWdBo5XnoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9SO7CUny9w5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('test5.json', 'w') as f:\n",
        "    json.dump(dt, f)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}